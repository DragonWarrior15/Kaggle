alpha  accuracy        roc
tfidf
0.001  0.84281494799   0.792332378184
0.01   0.852703223321  0.801827034115
0.1    0.854629510723  0.799457765859
1      0.79594195454   0.68603195022
10     0.679080518813  0.500599520384
100    0.678695261333  0.5

min_df = 5, tfidf
0.001  0.853088480801  0.80021666678
0.01   0.854115834082  0.801604922306
0.1    0.855913702324  0.804402684097
1      0.846025426994  0.778491719396
10     0.693591883909  0.523181454836
100    0.678695261333  0.5

SVC, tfidf
1.0    0.677539488892  0.509777041186
10.0   0.679465776294  0.514037436087
100.0  0.680878387055  0.516235677494

multinomialNB count vec, text length, word length, target rate
0.001  0.8518042892    0.82179040117
0.01   0.857326313086  0.827752707957
0.1    0.862719917812  0.834251803855
1.0    0.861820983691  0.830327336995
10.0   0.828945678695  0.745283319229
100.0  0.678695261333  0.5

multinomialNB tfidf min_df = 5 ngram_range=(2,4)
0.001  0.877616540388  0.839543691442
0.01   0.878515474509  0.845467580524
0.1    0.87979966611   0.849465404025
1.0    0.830743546937  0.745029331313
10.0   0.679722614614  0.501598721023
100.0  0.678695261333  0.5

multinomialNB tfidf with cleaning
0.001  0.841402337229  0.789397507538
0.01   0.850263259278  0.798451002679
0.1    0.856042121485  0.801235076272
1.0    0.803390265828  0.698780237872

multinomialNB count vec with cleaning
0.001  0.852831642481  0.82075830348
0.01   0.859381019648  0.829266426027
0.1    0.862976756132  0.834125320368
1.0    0.861820983691  0.82853838027

multinomialNB tfidf min_df = 5 ngram_range=(2,4) with cleaning
0.001  0.862719917812  0.81394188339
0.01   0.865930396815  0.82230533454
0.1    0.869269294979  0.829711065585
1.0    0.839732888147  0.762596053715

multinomialNB tfidf min_df = 5 ngram_range=(2,4) with cleaning, text length, word length, target rate
0.001  0.857839989726  0.808768311746
0.01   0.862591498652  0.819529844431
0.1    0.865930396815  0.831776281907
1.0    0.754590984975  0.620631101552

multinomialNB tfidf min_df = 5 ngram_range=(2,4) with cleaning wordnet
0.001  0.86079363041   0.809576255741
0.01   0.867214588417  0.822409546346
0.1    0.867728265057  0.826997285804
1.0    0.838448696546  0.760913350682

multinomialNB tfidf min_df = 5 ngram_range=(2,4) with cleaning wordnet, text length, word length, target rate
0.001  0.857839989726  0.807715984261
0.01   0.862463079491  0.818909073309
0.1    0.863490432773  0.827874086729
1.0    0.760241428021  0.629950230922

multinomialNB tfidf min_df = 5 ngram_range=(2,4) with cleaning wordnet, lower case
0.001  0.858096828047  0.807694733523
0.01   0.863875690253  0.819213125242
0.1    0.865159881854  0.824263276229
1.0    0.838705534866  0.762154892926


multinomialNB tfidf min_df = 5 ngram_range=(2,4) with cleaning wordnet, lower case
0.1,0.865159881854,0.824263276229
bot_20
part clean               0.095851
also wall thin           0.102126
standard practice        0.102408
nt heard anything        0.102715
small table two          0.106141
could told               0.106394
shine service            0.106534
shoe shine service       0.106534
small table two chair    0.107882
business practice        0.108028
nonalcoholic beverage    0.108354
another staff            0.108506
still around             0.108972
kitchenette small        0.109341
king bed asked           0.109760
rather full              0.110837
earlier would            0.110939
issue cleanliness        0.112075
lot walked               0.112085
inn based                0.112267
dtype: float64
top_20
accommodating friendly    1.000000
also good                 1.000000
beat location             1.000000
de la                     1.000000
great experience          1.000000
horrible bed              1.000000
hotel con                 1.000000
mouse room                1.000000
never leave               1.000000
nt stay                   1.000000
pleasant stay             1.000000
price great               1.000000
room small                1.000000
south park                1.000000
special mention           1.000000
staff friendly            1.000000
also towel                0.935692
cold cereal               0.889269
recommend definitely      0.877174
care comfort              0.863104
dtype: float64

multinomialNB tfidf min_df = 5, ngram_range=(2,4) with cleaning wordnet, lower case
0.1,0.857839989726,0.807189820518
bot_20
nodded          0.106599
ml              0.118009
hare            0.123911
screening       0.125195
remover         0.129561
mountainside    0.133142
pursue          0.138920
religious       0.141872
all             0.143073
chaperone       0.143532
radar           0.145330
reiterated      0.145839
relayed         0.145864
relented        0.146574
mocha           0.146707
aggravated      0.147918
handset         0.148118
coloured        0.148401
amazement       0.148707
aback           0.149606
dtype: float64
top_20
mouse           0.986492
impeccable      0.911938
thump           0.900890
sucked          0.885883
accomadating    0.870486
average         0.853777
diane           0.846344
de              0.838267
henry           0.826012
immensely       0.820568
jeffrey         0.816497
graceland       0.815671
great           0.814361
stayin          0.812333
pleasent        0.804723
lil             0.794842
peace           0.794549
amenties        0.792253
wyndham         0.788919
adolphus        0.787742
dtype: float64


multinomialNB tfidf min_df = 3, ngram_range=(1,3) with cleaning wordnet, lower case
0.1,0.874534480544,0.83706264884
bot_20
deep dark              0.053431
traveler never         0.054889
money instead          0.055052
small writing          0.055439
small writing desk     0.055439
suite large living     0.056116
problem every          0.056903
keeping eye            0.056980
spoke woman            0.058118
check status room      0.058402
item restocked         0.058457
service menu looked    0.058537
people away            0.058737
time showering         0.058781
completely separate    0.058831
area serve             0.058865
old post               0.059411
old post office        0.059411
friend join            0.060031
everything tasted      0.060698
dtype: float64
top_20
de                        0.816323
barry                     0.802899
mouse room                0.773402
thump                     0.762040
accommodating friendly    0.739082
cracker                   0.697321
horrible bed              0.690509
infected                  0.666485
nt stay                   0.658470
value night               0.656170
direction helpful         0.652989
diane                     0.651059
doll                      0.651001
stayed day hotel          0.643564
stayin                    0.627447
mouse                     0.625353
thing behind              0.618572
location room great       0.618159
beat location             0.618144
great place stay          0.616362
dtype: float64


LB 0.88899
Train 1,0.910643763044,0.887670588188
Test 1,0.892770001284,0.868070765715
negative words removed from stop words
tfVect1 = TfidfVectorizer(max_features=1800, ngram_range = (1,1))
tfVect2 = TfidfVectorizer(max_features=1000, ngram_range = (2,2))
tfVect3 = TfidfVectorizer(max_features=200, ngram_range = (1,1)) on only adjectives
X_train_tfidf = hstack((tfVect1.transform(X_train[:, 0]), tfVect2.transform(X_train[:, 0]), tfVect3.transform(X_train[:, 1])))
clf = LogisticRegression(penalty = 'l2', C = 1)
def cleaning_function(x):
    # x is a piece of text to be cleaned

    x = unicode(x, errors='ignore')
    x = x.lower()
    # print (x)
    x = word_tokenize(x)
    
    # x = [regex_punc.sub(u'', token) for token in x if not regex_punc.sub(u'', token) == u'']
    x = [regex_punc.sub(u'', token) for token in x]
    x = [regex_nt.sub(u'not', token) for token in x]
    x = list(filter(lambda x: x != u'', x))
    # x = [token for token in x if token not in stopwords.words('english')]
    x = list(filter(lambda word: word not in stopwords_set, x))
    # x = [porter.stem(token) for token in x]
    # x = list(map(porter.stem, x))
    x = list(map(wordnet.lemmatize, x))
    
    x = ' '.join(x)
    return (x)

LB 0.88885
Train 1,0.909744742334,0.886499959482
Test 1,0.894439450366,0.869405894395
negative words removed from stop words
tfVect1 = TfidfVectorizer(max_features=1800, ngram_range = (1,1))
tfVect2 = TfidfVectorizer(max_features=1000, ngram_range = (2,2))
X_train_tfidf = hstack((tfVect1.transform(X_train[:, 0]), tfVect2.transform(X_train[:, 0])))
clf = LogisticRegression(penalty = 'l2', C = 1)


LB 0.88962
Train 1,0.910354792101,0.887612285681
Test 1,0.895338384487,0.869331470894
negative words removed from stop words
df_train, df_dev = train_test_split(df.as_matrix(), test_size = 0.2, random_state = 442)
tfVect1 = TfidfVectorizer(max_features=1800, ngram_range = (1,1))
tfVect2 = TfidfVectorizer(max_features=1000, ngram_range = (2,2))
tfVect3 = TfidfVectorizer(max_features=200, ngram_range = (1,1)) on only adjectives
X_train_tfidf = hstack((tfVect1.transform(X_train[:, 0]), tfVect2.transform(X_train[:, 0]), tfVect3.transform(X_train[:, 1])))
clf = LogisticRegression(penalty = 'l2', C = 1)
def cleaning_function(x):
    # x is a piece of text to be cleaned

    x = unicode(x, errors='ignore')
    x = x.lower()
    # print (x)
    x = regex_horrible.sub(u' horrible ', x)
    x = regex_hyphen.sub(u'', x)
    x = word_tokenize(x)
    
    # x = [regex_punc.sub(u'', token) for token in x if not regex_punc.sub(u'', token) == u'']
    x = [regex_punc.sub(u'', token) for token in x]
    x = [regex_nt.sub(u'not', token) for token in x]
    x = list(filter(lambda x: x != u'', x))
    # x = [token for token in x if token not in stopwords.words('english')]
    x = list(filter(lambda word: word not in stopwords_set, x))
    # x = [porter.stem(token) for token in x]
    # x = list(map(porter.stem, x))
    x = list(map(wordnet.lemmatize, x))
    
    x = ' '.join(x)
    return (x)


LB 0.89239
Train 1,0.932926633489,0.914651100534
Test 1,0.902529857455,0.876951525389
negative words removed from stop words
df_train, df_dev = train_test_split(df.as_matrix(), test_size = 0.2, random_state = 442)
tfVect1 = TfidfVectorizer(max_features=3000, ngram_range = (1,1))
tfVect2 = TfidfVectorizer(max_features=4000, ngram_range = (2,2))
tfVect3 = TfidfVectorizer(max_features=3000, ngram_range = (1,1))  # only adjectives
X_train_tfidf = hstack((tfVect1.transform(X_train[:, 0]), tfVect2.transform(X_train[:, 0]), tfVect3.transform(X_train[:, 1])))
clf = LogisticRegression(penalty = 'l2', C = 1)
def cleaning_function(x):
    # x is a piece of text to be cleaned

    x = unicode(x, errors='ignore')
    x = x.lower()
    # print (x)
    x = regex_horrible.sub(u' horrible ', x)
    x = regex_hyphen.sub(u'', x)
    x = word_tokenize(x)
    
    # x = [regex_punc.sub(u'', token) for token in x if not regex_punc.sub(u'', token) == u'']
    x = [regex_punc.sub(u'', token) for token in x]
    x = [regex_nt.sub(u'not', token) for token in x]
    x = list(filter(lambda x: x != u'', x))
    # x = [token for token in x if token not in stopwords.words('english')]
    x = list(filter(lambda word: word not in stopwords_set, x))
    # x = [porter.stem(token) for token in x]
    # x = list(map(porter.stem, x))
    x = list(map(wordnet.lemmatize, x))
    
    x = ' '.join(x)
    return (x)

LB 0.89210
Train, 0.93411462514,0.916194435557
Test, 0.903557210736,0.878342448655
negative words removed from stop words
df_train, df_dev = train_test_split(df.as_matrix(), test_size = 0.2, random_state = 442)
tfVect1 = TfidfVectorizer(max_features=3600, ngram_range = (1,1))
tfVect2 = TfidfVectorizer(max_features=4000, ngram_range = (2,2))
tfVect3 = TfidfVectorizer(max_features=3500, ngram_range = (1,1))  # only adjectives
X_train_tfidf = hstack((tfVect1.transform(X_train[:, 0]), tfVect2.transform(X_train[:, 0]), tfVect3.transform(X_train[:, 1])))
clf = LogisticRegression(penalty = 'l2', C = 1)
def cleaning_function(x):
    # x is a piece of text to be cleaned

    x = unicode(x, errors='ignore')
    x = x.lower()
    # print (x)
    x = regex_horrible.sub(u' horrible ', x)
    x = regex_hyphen.sub(u'', x)
    x = word_tokenize(x)
    
    # x = [regex_punc.sub(u'', token) for token in x if not regex_punc.sub(u'', token) == u'']
    x = [regex_punc.sub(u'', token) for token in x]
    x = [regex_nt.sub(u'not', token) for token in x]
    x = list(filter(lambda x: x != u'', x))
    # x = [token for token in x if token not in stopwords.words('english')]
    x = list(filter(lambda word: word not in stopwords_set, x))
    # x = [porter.stem(token) for token in x]
    # x = list(map(porter.stem, x))
    x = list(map(wordnet.lemmatize, x))
    
    x = ' '.join(x)
    return (x)