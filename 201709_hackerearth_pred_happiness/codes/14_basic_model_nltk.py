import common_vars as c_vars
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from datetime import datetime
import re
import string
import pickle
from multiprocessing import Pool

from nltk import pos_tag
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem.snowball import SnowballStemmer
from nltk.stem.wordnet import WordNetLemmatizer

import sys
import gc

from scipy import sparse
from scipy.stats import pearsonr
from sklearn.feature_selection import chi2
from scipy.sparse import hstack, coo_matrix
from scipy.sparse import csr_matrix

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_selection import RFE

from sklearn.naive_bayes import MultinomialNB, GaussianNB
from sklearn.svm import SVC
from sklearn.metrics import roc_auc_score, accuracy_score
from sklearn.ensemble import RandomForestClassifier
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier

# codes for compatibility between python 2 and 3
try:
    reload(sys)
except NameError:
    pass
try:
    sys.setdefaultencoding('utf8')
except AttributeError:
    pass

row_ind = 0

regex_punc = re.compile('[%s]' % re.escape(string.punctuation))
regex_nt = re.compile('nt')
regex_quot = re.compile('["]+')
regex_hyphen = re.compile('[-]+(st|nd|rd|th)?')

stopwords_set = set(stopwords.words('english')) - set(['but','if','as','until','while','of','against','again','then','once','no','nor','not','only','too','very','should','ain','aren','couldn','didn','doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan','shouldn','wasn','weren','won',])
porter = PorterStemmer()
snowball = SnowballStemmer('english')
wordnet = WordNetLemmatizer()
num_partitions = 2 #number of partitions to split dataframe
num_cores = 2 #number of cores on your machine

def cleaning_function(x):
    # x is a piece of text to be cleaned

    try:
        x = unicode(x, errors='ignore')
    except NameError:
        pass
    x = x.lower()
    # print (x)
    x = regex_quot.sub(u'', x)
    x = regex_hyphen.sub(u'', x)
    x = word_tokenize(x)
    
    # x = [regex_punc.sub(u'', token) for token in x if not regex_punc.sub(u'', token) == u'']
    x = [regex_punc.sub(u'', token) for token in x]
    x = [regex_nt.sub(u'not', token) for token in x]
    x = list(filter(lambda x: x != u'', x))
    # x = [token for token in x if token not in stopwords.words('english')]
    x = list(filter(lambda word: word not in stopwords_set, x))
    # x = [porter.stem(token) for token in x]
    # x = list(map(porter.stem, x))
    x = list(map(wordnet.lemmatize, x))
    
    x = ' '.join(x)
    return (x)

def pos_tag_adj_extract(x):
    x = pos_tag(word_tokenize(x))
    x = list(filter(lambda word: 'JJ' in word[1], x))
    x = ' '.join(list(map(lambda word: word[0], x)))
    return (x)

def parallelize_dataframe(df, func):
    df_split = np.array_split(df, num_partitions)
    pool = Pool(num_cores)
    df = pd.concat(pool.map(func, df_split))
    pool.close()
    pool.join()
    return (df)

def parallel_func_to_apply(data):
    data['Description_Clean'] = data['Description'].apply(lambda x: cleaning_function(x))
    data['Description_Clean_Adj'] = data['Description_Clean'].apply(lambda x: pos_tag_adj_extract(x))
    return (data)

def main():
    
    # df = pd.read_csv(c_vars.train_file)
    df = pd.read_csv(c_vars.train_file_processed, encoding = "ISO-8859-1")
    df['Description_Clean'].fillna('', inplace = True, axis = 0)
    df['Description_Clean_Adj'].fillna('', inplace = True, axis = 0)
    '''
    df.drop(['User_ID'], axis = 1, inplace = True)
    df['Is_Response'] = df['Is_Response'].apply(lambda x: 1 if x == 'happy' else 0)
    df['Browser_Used'] = df['Browser_Used'].apply(lambda x: c_vars.browser_dict[x])

    # print ('Cleaning started at ' + str(datetime.now()))
    df = parallelize_dataframe(df, parallel_func_to_apply)
    # print ('Cleaning complete at ' + str(datetime.now()))
    df.to_csv(c_vars.train_file_processed, index = False)
    
    df_submit = pd.read_csv(c_vars.test_file)
    df_submit['Browser_Used'] = df_submit['Browser_Used'].apply(lambda x: c_vars.browser_dict[x])
    df_submit = parallelize_dataframe(df_submit, parallel_func_to_apply)
    df_submit.to_csv(c_vars.test_file_processed, index = False)
    # sys.exit()
    '''
    df['text_length'] = df['Description_Clean'].apply(lambda x: len(x))
    df['word_count'] = df['Description_Clean'].apply(lambda x: len(x.split(' ')))

    # create more copies of the unhappy/bad reviews to identify those words
    # df = pd.concat([df, df.loc[df['Is_Response'] == 0,], df.loc[df['Is_Response'] == 0,]])

    df_train, df_dev = train_test_split(df.as_matrix(), test_size = 0.2, random_state = 442)
    df_train = pd.DataFrame(df_train, columns = c_vars.header_useful + ['Description_Clean', 'Description_Clean_Adj', 'text_length', 'word_count'])
    df_dev = pd.DataFrame(df_dev, columns = c_vars.header_useful + ['Description_Clean', 'Description_Clean_Adj', 'text_length', 'word_count'])

    df_device = df_train.groupby(['Device_Used'])['Is_Response'].agg(['count', np.sum])
    df_device.reset_index(inplace = True)
    # print (df_device.columns.values)
    # df_device.columns = df_device.columns.get_level_values(0)
    df_device['target_rate'] = df_device['sum']/df_device['count']
    df_device = df_device[['Device_Used', 'target_rate']]

    df_train = pd.merge(df_train, df_device, how = 'left', on = 'Device_Used', suffixes = ('', ''))
    df_dev = pd.merge(df_dev, df_device, how = 'left', on = 'Device_Used', suffixes = ('', ''))

    X_train = df_train[['Description_Clean', 'Description_Clean_Adj', 'text_length', 'word_count', 'target_rate']].as_matrix()
    X_dev = df_dev[['Description_Clean', 'Description_Clean_Adj', 'text_length', 'word_count', 'target_rate']].as_matrix()
    y_train = df_train['Is_Response'].as_matrix().astype(np.int64)
    y_dev = df_dev['Is_Response'].as_matrix().astype(np.int64)

    # print ('X_train ' + str(X_train.shape))
    # print ('X_dev '   + str(X_dev.shape))
    # print ('y_train ' + str(y_train.shape))
    # print ('y_dev '   + str(y_dev.shape))

    # print (X_train[1,:])

    # tfVect = TfidfVectorizer(min_df = 5, ngram_range = (2, 4))
    # tfVect = TfidfVectorizer(min_df = 3, ngram_range = (1, 3))
    tfVect1 = TfidfVectorizer(min_df = 10, ngram_range = (1,1))
    tfVect2 = TfidfVectorizer(min_df = 10, ngram_range = (2,2))
    tfVect3 = TfidfVectorizer(min_df = 10, ngram_range = (1,1))
    # countVect = CountVectorizer()
    # tfVect = TfidfVectorizer(min_df = 5)

    tfVect1.fit(X_train[:, 0])
    tfVect2.fit(X_train[:, 0])
    tfVect3.fit(X_train[:, 1])
    # countVect.fit(X_train[:, 1])
    X_train_tfidf = hstack((tfVect1.transform(X_train[:, 0]), tfVect2.transform(X_train[:, 0]), tfVect3.transform(X_train[:, 1])))

    # truncatedsvd = TruncatedSVD(n_components = 500, random_state = 42)
    # truncatedsvd.fit(X_train_tfidf)
    # X_train_tfidf = truncatedsvd.transform(X_train_tfidf)
    X_train_tfidf = c_vars.add_feature(X_train_tfidf, X_train[:, 2].astype(np.float64))
    X_train_tfidf = c_vars.add_feature(X_train_tfidf, X_train[:, 3].astype(np.int64))
    X_train_tfidf = c_vars.add_feature(X_train_tfidf, X_train[:, 4].astype(np.int64))
    '''
    # get the correlations
    f = open('../analysis/corr_coef.csv','w')
    # for i in range(X_train_tfidf.shape[1]):
    for i in range(X_train_tfidf.shape[1] - 3):
        if i < len(tfVect1.vocabulary_):
            tfVect = tfVect1
            j = i
        elif i < len(tfVect1.vocabulary_) + len(tfVect2.vocabulary_):
            tfVect = tfVect2
            j = i - len(tfVect1.vocabulary_)
        else:
            tfVect = tfVect3
            j = i - (len(tfVect1.vocabulary_) + len(tfVect2.vocabulary_))
        for k in tfVect.vocabulary_:
            if tfVect.vocabulary_[k] == j:
                term = k
                break
        f.write(str(i) + ',' + str(term) + ',' + str(np.corrcoef(X_train_tfidf.tocsr()[:, i].todense().reshape(y_train.shape[0],), y_train)[0,1]) + '\n')

    f.write(str(X_train_tfidf.shape[1] - 3) + ',' + str('text_length') + ',' + str(np.corrcoef(X_train_tfidf.tocsr()[:, X_train_tfidf.shape[1] - 3].todense().reshape(y_train.shape[0],), y_train)[0,1]) + '\n')
    f.write(str(X_train_tfidf.shape[1] - 2) + ',' + str('word_count')  + ',' + str(np.corrcoef(X_train_tfidf.tocsr()[:, X_train_tfidf.shape[1] - 2].todense().reshape(y_train.shape[0],), y_train)[0,1]) + '\n')
    f.write(str(X_train_tfidf.shape[1] - 1) + ',' + str('target_rate') + ',' + str(np.corrcoef(X_train_tfidf.tocsr()[:, X_train_tfidf.shape[1] - 1].todense().reshape(y_train.shape[0],), y_train)[0,1]) + '\n')
    
    f.close()
    sys.exit()
    '''
    X_dev_tfidf = hstack((tfVect1.transform(X_dev[:, 0]), tfVect2.transform(X_dev[:, 0]), tfVect3.transform(X_dev[:, 1])))
    # X_dev_tfidf = truncatedsvd.transform(X_dev_tfidf)
    X_dev_tfidf = c_vars.add_feature(X_dev_tfidf, X_dev[:, 2].astype(np.float64))
    X_dev_tfidf = c_vars.add_feature(X_dev_tfidf, X_dev[:, 3].astype(np.int64))
    X_dev_tfidf = c_vars.add_feature(X_dev_tfidf, X_dev[:, 4].astype(np.int64))

    # select specific columns to use in the model
    # cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,561,5033,7987,5332]
    # cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,561,5033,7987,5332,6236,911,7341,8186,3499,27140,985,1021,429,5508,7374,1886,1290,5035,26669,7477,303,6932,4308,6723,40339,1020,38432,15157,6792,6583,32473,6840,4845]
    # cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,5033,8186,985,429,40339,6583]
    # cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6269,7915,2535,5033,8186,985,40339]
    cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,561,5033,7987,5332,6236,911,7341,8186,3499,27140,985,1021,429,5508,7374,1886,1290,5035,26669,7477,303,6932,4308,6723,40339,1020,38432,15157,6792,6583,32473,6840,4845,4939,27146,2068,2765,6108,3422,6543,2762,1179,3277,6933,6724,5888,26960,25468,38437,2414,26306,3602,249,7454,20305,3544,8206,2148,180,6687,4297,5755,650,4255,4142,9242,5292,18445,3270,2293,1714,5231,34454,1274,14688,3080,7694,18378,2672,7604,8210,12549,5727,5203,5379,7531,35031,4196,12215,245,4676,1816,7768,538,7011,25282,1064,5125,19,1641,4874,4397,5940,7750,7630,4861,38464,1291,1960,26594,2102,4283,2515,5314,39059,1200,3982,23671,7928,7985,7119,3776,944,8072,5948,6915,721,595,7384,26741,4771,4309,9221,28319,40255,4669,3458,38489,26593,3015,6733,7174,5326,13484,40413,6574,5052,4090,38505,22087,5482,4688,28697,26330,4733,4076,4732,1441,8196,38619,38559,7980,5034,6449,2360,341,37227,2329,1261,34955,27111,37206,4901,7314,2016,5294,5684,337,2040,4307,3227,8154,25550,37208,4134,3104,783,6519,7944,2348,719,34990,4282,357,562,30000,523,32487,27142,1302,8173,6799,1301,32642,1898,875,6737,7568,31329,3723,25609,38557,5646,2359,4913,6978,5208,34328,4396,8199,7733,626,6592,3932,8151,20781,8264,32187,4715,26379,1824,7442,7459,1295,4919,2982,32935,15149,1170,737,2514,6726,35797,29130,38577,35214,6815,8692,20486,1178,5509,4966,5464,2266,37060,4790,26173,7966,32720,4163,5351,14544,7363,2030,7728,2258,15328,19574,26472,13490,39095,25369,1319,34353,35448,27194,27001,20959,39683,1669,26983,10806,6619,17710,25561,26733,1482,7080,4257,6448,4647,13978,614,2501,40382,1194,5140,5657,537,32761,34321,6075,8035,33713,38604,14564,4651,25530,28001,25018,34200,1397,6849,6275,3311,1775,10716,20941,38496,5167,4025,2182,2041,4498,7678,40057,7386,7989,32460,8179,2394,19588,23492,19622,6645]
    cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,561,5033,7987,5332,6236,911,7341,8186,3499,27140,985,1021,429,5508,7374,1886,1290,5035,26669,7477,303,6932,4308,6723,40339,1020,38432,15157,6792,6583,32473,6840,4845,4939,27146,2068,2765,6108,3422,6543,2762,1179,3277,6933,6724,5888,26960,25468,38437,2414,26306,3602,249,7454,20305,3544,8206,2148,180,6687,4297,5755,650,4255,4142,9242,5292,18445,3270,2293,1714,5231,34454,1274,14688,3080,7694,18378,2672,7604,8210,12549,5727,5203,5379,7531,35031,4196,12215,245,4676,1816,7768,538,7011,25282,1064,5125,19,1641,4874,4397,5940,7750,7630,4861,38464,1291,1960,26594,2102,4283,2515,5314,39059,1200,3982,23671,7928,7985,7119,3776,944,8072,5948,6915,721,595,7384,26741,4771,4309,9221,28319,40255,4669,3458,38489,26593,3015,6733,7174,5326,13484,40413,6574,5052,4090,38505,22087,5482,4688,28697,26330,4733,4076,4732,1441,8196,38619,38559,7980,5034,6449,2360,341,37227,2329,1261,34955,27111,37206,4901,7314,2016,5294,5684,337,2040,4307,3227,8154,25550,37208,4134,3104,783,6519,7944,2348,719,34990,4282,357,562,30000,523,32487,27142,1302,8173,6799,1301,32642,1898,875,6737,7568,31329,3723,25609,38557,5646,2359,4913,6978,5208,34328,4396,8199,7733,626,6592,3932,8151,20781,8264,32187,4715,26379,1824,7442,7459,1295,4919,2982,32935,15149,1170,737,2514,6726,35797,29130,38577,35214,6815,8692,20486,1178,5509,4966,5464,2266,37060,4790,26173,7966,32720,4163,5351,14544,7363,2030,7728,2258,15328,19574,26472,13490,39095,25369,1319,34353,35448,27194,27001,20959,39683,1669,26983,10806,6619,17710,25561,26733,1482,7080,4257,6448,4647,13978,614,2501,40382,1194,5140,5657,537,32761,34321,6075,8035,33713,38604,14564,4651,25530,28001,25018,34200,1397,6849,6275,3311,1775,10716,20941,38496,5167,4025,2182,2041,4498,7678,40057,7386,7989,32460,8179,2394,19588,23492,19622,6645,5891,15153,4253,2877,6170,6033,5530,7606,12308,27364,15495,32465,19665,3152,32716,8691,6195,12458,5067,26841,6898,40446,32305,19316,21725,10513,2039,4597,2507,819,38726,17565,5393,8053,1171,8230,37216,37191,2630,765,3246,8028,39532,6451,26576,7232,2249,5010,32658,12209,28683,2785,6441,26745,24732,3002,34589,26678,3249,38563,32915,12498,26805,33084,21348,1598,24850,3355,32050,38591,17558,7385,40257,28054,1867,25411,32489,6591,6571,27014,1784,4816,8071,6326,7778,6106,7515,1143,3135,7019,3810,754,19647,6981,3306,17698,6973,7744,15317,26542,475,787,6099,6359,7265,7343,16313,1451,7645,2555,1317,30194,12940,28748,1361,6710,2319,4847,7358,12889,38646,4897,4373,4485,5329,2510,1481,1662,2660,44,6358,4987,10553,1455,1682,6604,38574,4943,4678,28663,7161,6141,35229,22441,7677,21264,35284,2268,3539,2945,476,613,5471,27039,1493,4862,968,32916,19424,15299,33080,7143]
    cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,561,5033,7987,5332,6236,911,7341,8186,3499,27140,985,1021,429,5508,7374,1886,1290,5035,26669,7477,303,6932,4308,6723,40339,1020,38432,15157,6792,6583,32473,6840,4845,4939,27146,2068,2765,6108,3422,6543,2762,1179,3277,6933,6724,5888,26960,25468,38437,2414,26306,3602,249,7454,20305,3544,8206,2148,180,6687,4297,5755,650,4255,4142,9242,5292,18445,3270,2293,1714,5231,34454,1274,14688,3080,7694,18378,2672,7604,8210,12549,5727,5203,5379,7531,35031,4196,12215,245,4676,1816,7768,538,7011,25282,1064,5125,19,1641,4874,4397,5940,7750,7630,4861,38464,1291,1960,26594,2102,4283,2515,5314,39059,1200,3982,23671,7928,7985,7119,3776,944,8072,5948,6915,721,595,7384,26741,4771,4309,9221,28319,40255,4669,3458,38489,26593,3015,6733,7174,5326,13484,40413,6574,5052,4090,38505,22087,5482,4688,28697,26330,4733,4076,4732,1441,8196,38619,38559,7980,5034,6449,2360,341,37227,2329,1261,34955,27111,37206,4901,7314,2016,5294,5684,337,2040,4307,3227,8154,25550,37208,4134,3104,783,6519,7944,2348,719,34990,4282,357,562,30000,523,32487,27142,1302,8173,6799,1301,32642,1898,875,6737,7568,31329,3723,25609,38557,5646,2359,4913,6978,5208,34328,4396,8199,7733,626,6592,3932,8151,20781,8264,32187,4715,26379,1824,7442,7459,1295,4919,2982,32935,15149,1170,737,2514,6726,35797,29130,38577,35214,6815,8692,20486,1178,5509,4966,5464,2266,37060,4790,26173,7966,32720,4163,5351,14544,7363,2030,7728,2258,15328,19574,26472,13490,39095,25369,1319,34353,35448,27194,27001,20959,39683,1669,26983,10806,6619,17710,25561,26733,1482,7080,4257,6448,4647,13978,614,2501,40382,1194,5140,5657,537,32761,34321,6075,8035,33713,38604,14564,4651,25530,28001,25018,34200,1397,6849,6275,3311,1775,10716,20941,38496,5167,4025,2182,2041,4498,7678,40057,7386,7989,32460,8179,2394,19588,23492,19622,6645,5891,15153,4253,2877,6170,6033,5530,7606,12308,27364,15495,32465,19665,3152,32716,8691,6195,12458,5067,26841,6898,40446,32305,19316,21725,10513,2039,4597,2507,819,38726,17565,5393,8053,1171,8230,37216,37191,2630,765,3246,8028,39532,6451,26576,7232,2249,5010,32658,12209,28683,2785,6441,26745,24732,3002,34589,26678,3249,38563,32915,12498,26805,33084,21348,1598,24850,3355,32050,38591,17558,7385,40257,28054,1867,25411,32489,6591,6571,27014,1784,4816,8071,6326,7778,6106,7515,1143,3135,7019,3810,754,19647,6981,3306,17698,6973,7744,15317,26542,475,787,6099,6359,7265,7343,16313,1451,7645,2555,1317,30194,12940,28748,1361,6710,2319,4847,7358,12889,38646,4897,4373,4485,5329,2510,1481,1662,2660,44,6358,4987,10553,1455,1682,6604,38574,4943,4678,28663,7161,6141,35229,22441,7677,21264,35284,2268,3539,2945,476,613,5471,27039,1493,4862,968,32916,19424,15299,33080,7143,6044,1940,34407,6488,7175,6951,7508,32488,651,2837,38416,5822,5462,485,159,29260,4286,5671,7825,22920,19694,6794,6939,20974,2540,6077,34881,3550,6966,842,33755,37198,2959,8086,17624,7856,18423,6959,36598,20215,525,1494,6395,29645,23634,15240,4042,26261,31112,28241,6367,9783,23388,5449,12216,2382,11374,32164,3141,26761,1242,32379,20159,6270,8068,6944,4857,29525,34902,2699,32257,7162,32097,19690,1479,8215,7730,7400,34331,24279,40254,34484,16651,38535,20306,8069,39706,5495,3656,1028,18775,1472,29957,15484,3039,7810,2798,7043,12743,19642,1580,33677,2769,6823,1280,19663,6281,26764,200,1346,10498,28734,38462,15155,1576,6743,1085,35100,3401,347,5918,3606,25988,3259,24285,25781,7704,9763,593,1238,5651,32641,1718,2716,30281,32900,6173,16675,25522,2988,3623,6283,23837,3858,3638,6638,10590,4903,667,23625,37164,5978,26541,4863,38617,6290,29548,29980,37791,3463,2578,32996,12393,13143,749,17399,26537,13352,3455,277,3150,15411,1292,2375,6016,4439,12853,7773,7013,4322,17169,6577,684,4512,38515,26470,1128,26952,2646,588,24996,12753,26988,1799,22272,32674,7642,33462,20019,2170,25566,26645,30174,4638,26783,11755,4879,7315,1880,3533,6890,8250,2625,2853,3303,4956,33076,5461,6180,7855,7968,34890,6599,6053,1213,2689,3694,1835,2696,2886,7800,1681,491,26284,36369,5397,38456,5012,21648,26207,6356,4763,27862,1803,2413,8891,3641,32087,26602,32100,6995,37896,27453,5028,28723,6178,39659,16900,37373,21167,7090,19544,13620,2067,29010,37457,27174,13597,454,1543,36280,33334,28138,85,39699,24765,19638,31374,26653,5752,10872,27979,4392,27357,522,34551,5334,20209,2667,1225,12755,16228,2018,5647,38465,1678,32109,3208,10499,40335,37321,20709,26378,21011,40403,5823,6169,38625,4313,4125,3362,4682,24,2801,563,15487,13354,6019,27175,28999,30046,5582,1038,19497,6657,3667,27832,6431,4478,629,14281,21470,7814,15686,15125,2593,1964,12535,8180,25145,2476,2596,32588,26755,4935,19678,817,28205,4551,12547,3059,32405,28568,7669,27061,3233,3273,18937,35046,3464,24995,17155,10983,8276,4407,32464,4137,25064,908,33051,40088,18795,724,34473,27452,30483,30191,1328,19236,18803,40256,25460,4906,5890,17774,25641,38418,25459,39650,34815,32262,12539,6428,17261,7946,488,6761,170,2933,6062,36620,467,39995,26700,40318,33362,13579,26724,7173,5469,4181,37163,15298,1830,7868,5068,39663,4744,1434,28341,4328,37650,25818,28213,1316,11436,7405,25428,38502,26016,20369,6018,4552,2767,3797,37232,22271,3285,16042,7108,585,313,14505,1854,26466,26311,3305,414,27659,32069,12579,8613,48,31086,12000,13556,32331,3386,1700,2548,335,28451,32366,1962,4258,4893,4390,1097,28360,6036,4136,8358,35796,2529,17165,835,3680,3665,5242,28047,40468,24289,1324,27280,40454,16043]
    cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,561,5033,7987,5332,6236,911,7341,8186,3499,27140,985,1021,429,5508,7374,1886,1290,5035,26669,7477,303,6932,4308,6723,40339,1020,38432,15157,6792,6583,32473,6840,4845,4939,27146,2068,2765,6108,3422,6543,2762,1179,3277,6933,6724,5888,26960,25468,38437,2414,26306,3602,249,7454,20305,3544,8206,2148,180,6687,4297,5755,650,4255,4142,9242,5292,18445,3270,2293,1714,5231,34454,1274,14688,3080,7694,18378,2672,7604,8210,12549,5727,5203,5379,7531,35031,4196,12215,245,4676,1816,7768,538,7011,25282,1064,5125,19,1641,4874,4397,5940,7750,7630,4861,38464,1291,1960,26594,2102,4283,2515,5314,39059,1200,3982,23671,7928,7985,7119,3776,944,8072,5948,6915,721,595,7384,26741,4771,4309,9221,28319,40255,4669,3458,38489,26593,3015,6733,7174,5326,13484,40413,6574,5052,4090,38505,22087,5482,4688,28697,26330,4733,4076,4732,1441,8196,38619,38559,7980,5034,6449,2360,341,37227,2329,1261,34955,27111,37206,4901,7314,2016,5294,5684,337,2040,4307,3227,8154,25550,37208,4134,3104,783,6519,7944,2348,719,34990,4282,357,562,30000,523,32487,27142,1302,8173,6799,1301,32642,1898,875,6737,7568,31329,3723,25609,38557,5646,2359,4913,6978,5208,34328,4396,8199,7733,626,6592,3932,8151,20781,8264,32187,4715,26379,1824,7442,7459,1295,4919,2982,32935,15149,1170,737,2514,6726,35797,29130,38577,35214,6815,8692,20486,1178,5509,4966,5464,2266,37060,4790,26173,7966,32720,4163,5351,14544,7363,2030,7728,2258,15328,19574,26472,13490,39095,25369,1319,34353,35448,27194,27001,20959,39683,1669,26983,10806,6619,17710,25561,26733,1482,7080,4257,6448,4647,13978,614,2501,40382,1194,5140,5657,537,32761,34321,6075,8035,33713,38604,14564,4651,25530,28001,25018,34200,1397,6849,6275,3311,1775,10716,20941,38496,5167,4025,2182,2041,4498,7678,40057,7386,7989,32460,8179,2394,19588,23492,19622,6645,5891,15153,4253,2877,6170,6033,5530,7606,12308,27364,15495,32465,19665,3152,32716,8691,6195,12458,5067,26841,6898,40446,32305,19316,21725,10513,2039,4597,2507,819,38726,17565,5393,8053,1171,8230,37216,37191,2630,765,3246,8028,39532,6451,26576,7232,2249,5010,32658,12209,28683,2785,6441,26745,24732,3002,34589,26678,3249,38563,32915,12498,26805,33084,21348,1598,24850,3355,32050,38591,17558,7385,40257,28054,1867,25411,32489,6591,6571,27014,1784,4816,8071,6326,7778,6106,7515,1143,3135,7019,3810,754,19647,6981,3306,17698,6973,7744,15317,26542,475,787,6099,6359,7265,7343,16313,1451,7645,2555,1317,30194,12940,28748,1361,6710,2319,4847,7358,12889,38646,4897,4373,4485,5329,2510,1481,1662,2660,44,6358,4987,10553,1455,1682,6604,38574,4943,4678,28663,7161,6141,35229,22441,7677,21264,35284,2268,3539,2945,476,613,5471,27039,1493,4862,968,32916,19424,15299,33080,7143,6044,1940,34407,6488,7175,6951,7508,32488,651,2837,38416,5822,5462,485,159,29260,4286,5671,7825,22920,19694,6794,6939,20974,2540,6077,34881,3550,6966,842,33755,37198,2959,8086,17624,7856,18423,6959,36598,20215,525,1494,6395,29645,23634,15240,4042,26261,31112,28241,6367,9783,23388,5449,12216,2382,11374,32164,3141,26761,1242,32379,20159,6270,8068,6944,4857,29525,34902,2699,32257,7162,32097,19690,1479,8215,7730,7400,34331,24279,40254,34484,16651,38535,20306,8069,39706,5495,3656,1028,18775,1472,29957,15484,3039,7810,2798,7043,12743,19642,1580,33677,2769,6823,1280,19663,6281,26764,200,1346,10498,28734,38462,15155,1576,6743,1085,35100,3401,347,5918,3606,25988,3259,24285,25781,7704,9763,593,1238,5651,32641,1718,2716,30281,32900,6173,16675,25522,2988,3623,6283,23837,3858,3638,6638,10590,4903,667,23625,37164,5978,26541,4863,38617,6290,29548,29980,37791,3463,2578,32996,12393,13143,749,17399,26537,13352,3455,277,3150,15411,1292,2375,6016,4439,12853,7773,7013,4322,17169,6577,684,4512,38515,26470,1128,26952,2646,588,24996,12753,26988,1799,22272,32674,7642,33462,20019,2170,25566,26645,30174,4638,26783,11755,4879,7315,1880,3533,6890,8250,2625,2853,3303,4956,33076,5461,6180,7855,7968,34890,6599,6053,1213,2689,3694,1835,2696,2886,7800,1681,491,26284,36369,5397,38456,5012,21648,26207,6356,4763,27862,1803,2413,8891,3641,32087,26602,32100,6995,37896,27453,5028,28723,6178,39659,16900,37373,21167,7090,19544,13620,2067,29010,37457,27174,13597,454,1543,36280,33334,28138,85,39699,24765,19638,31374,26653,5752,10872,27979,4392,27357,522,34551,5334,20209,2667,1225,12755,16228,2018,5647,38465,1678,32109,3208,10499,40335,37321,20709,26378,21011,40403,5823,6169,38625,4313,4125,3362,4682,24,2801,563,15487,13354,6019,27175,28999,30046,5582,1038,19497,6657,3667,27832,6431,4478,629,14281,21470,7814,15686,15125,2593,1964,12535,8180,25145,2476,2596,32588,26755,4935,19678,817,28205,4551,12547,3059,32405,28568,7669,27061,3233,3273,18937,35046,3464,24995,17155,10983,8276,4407,32464,4137,25064,908,33051,40088,18795,724,34473,27452,30483,30191,1328,19236,18803,40256,25460,4906,5890,17774,25641,38418,25459,39650,34815,32262,12539,6428,17261,7946,488,6761,170,2933,6062,36620,467,39995,26700,40318,33362,13579,26724,7173,5469,4181,37163,15298,1830,7868,5068,39663,4744,1434,28341,4328,37650,25818,28213,1316,11436,7405,25428,38502,26016,20369,6018,4552,2767,3797,37232,22271,3285,16042,7108,585,313,14505,1854,26466,26311,3305,414,27659,32069,12579,8613,48,31086,12000,13556,32331,3386,1700,2548,335,28451,32366,1962,4258,4893,4390,1097,28360,6036,4136,8358,35796,2529,17165,835,3680,3665,5242,28047,40468,24289,1324,27280,40454,16043,32290,25422,14056,34319,7009,7731,33576,22962,27000,19100,647,23628,1864,40472,8361,1569,3670,27109,15210,160,39968,17701,38667,3221,230,7342,6489,1901,1620,10223,26265,3364,22323,26363,11023,3318,6715,35051,37967,2880,6188,12745,3358,21214,5851,32662,4832,38868,8097,40439,2057,15500,7898,2235,4324,32822,11110,34708,19949,32260,296,3037,6562,29470,26550,6494,6444,9973,10205,26546,3331,5942,6968,2364,7619,12551,13663,33199,24254,461,1181,24874,6728,323,6299,38566,6467,11244,11438,2641,7693,690,15346,32192,8653,2138,1443,11036,5341,5403,16847,39110,6899,4365,25679,7051,18495,4642,6470,29682,38594,27990,2541,20577,7798,5800,1205,1572,26494,8062,32917,5463,27176,1422,3640,10201,36616,16792,2109,9607,33174,28355,6280,24269,32804,32943,17720,6273,2780,7565,8936,10221,745,2855,2952,26877,8177,28556,4060,10009,36494,28398,39103,1119,15264,2538,10837,38669,5610,5487,733,4986,27118,19387,5868,14890,27114,4348,29648,32615,9957,28376,8019,38848,32134,25563,20976,10767,32412,20061,25419,37212,28676,4044,12596,6727,1394,529,24293,5163,34405,15130,6838,27830,40514,5436,3513,520,531,11060,4023,37202,4300,8935,9971,40423,30196,38455,15493,21581,26707,5643,975,38839,584,2342,1440,40366,12509,32092,34273,12752,8569,7879,37125,15034,6390,3715,6050,14726,660,13002,36544,1918,16606,4965,22748,3315,10104,33403,22626,17676,33097,8650,4124,25299,361,7263,28083,1199,237,3402,26914,4497,30472,30690,24986,26348,910,16014,7387,7461,2207,3103,23438,32517,6851,13616,32568,35156,5118,610,26496,21778,9235,3366,38864,7030,1040,1892,23564,3292,3504,2502,11027,20440,32453,3861,5474,5263,27067,164,8231,9220,6566,1825,21357,6618,19874,40116,26735,32527,27123,32330,15557,7743,39260,38415,10812,1630,37209,8194,12,10870,9741,58,4908,33810,29011,3864,6338,25309,1347,5051,23191,39674,4958,19488,2412,4343,27545,8216,4126,7034,37285,26830,9768,36609,5869,4585,20901,19017,7369,19915,16371,26667,13598,26559,5494,6778,129,24089,27869,20441,6168,34420,422,39261,11944,5876,29850,5857,5544,3241,6612,38444,7447,15591,3719,40182,17658,34876,31552,4224,136,33682,7311,7288,26638,12480,2664,7742,6822,17690,27050,24887,26985,950,16346,4029,772,32747,666,3764,28029,37114,16909,256,35778,10192,1163,519,4945,19553,17880,6529,4589,6683,31850,13165,1833,9695,28391,35109,24005,3650,8182,14094,20809,2730,7803,32936,19199,17162,3356,27185,6391,1693,32120,16880,1913,31382,4563,14069,32731,161,18927,1315,27558,32225,38139,34419,26172,20953,20248,13531,6293,30170,1959,26958,40397,34476,8163,34912,29696,34323,7834,30189,345,301,39084,6241,15691,37330,32046,6052,2017,659,6767,26583,673,13708,384,27097,31434,16330,7584,320,32103,5606,33015,25458,6300,7941,26165,26971,6857,36737,29032,38638,123,26197,13985,27072,8162,32702,7388,2661,21761,8070,6100,1646,1846,3286,331,1134,23744,13802,36753,314,18792,37903,10717]
    cols_to_use = [4932,3197,7483,2032,4890,6926,40418,1391,3387,1083,2973,27037,8211,6310,6269,7915,2535,561,5033,7987,5332,6236,911,7341,8186,3499,27140,985,1021,429,5508,7374,1886,1290,5035,26669,7477,303,6932,4308,6723,40339,1020,38432,15157,6792,6583,32473,6840,4845,4939,27146,2068,2765,6108,3422,6543,2762,1179,3277,6933,6724,5888,26960,25468,38437,2414,26306,3602,249,7454,20305,3544,8206,2148,180,6687,4297,5755,650,4255,4142,9242,5292,18445,3270,2293,1714,5231,34454,1274,14688,3080,7694,18378,2672,7604,8210,12549,5727,5203,5379,7531,35031,4196,12215,245,4676,1816,7768,538,7011,25282,1064,5125,19,1641,4874,4397,5940,7750,7630,4861,38464,1291,1960,26594,2102,4283,2515,5314,39059,1200,3982,23671,7928,7985,7119,3776,944,8072,5948,6915,721,595,7384,26741,4771,4309,9221,28319,40255,4669,3458,38489,26593,3015,6733,7174,5326,13484,40413,6574,5052,4090,38505,22087,5482,4688,28697,26330,4733,4076,4732,1441,8196,38619,38559,7980,5034,6449,2360,341,37227,2329,1261,34955,27111,37206,4901,7314,2016,5294,5684,337,2040,4307,3227,8154,25550,37208,4134,3104,783,6519,7944,2348,719,34990,4282,357,562,30000,523,32487,27142,1302,8173,6799,1301,32642,1898,875,6737,7568,31329,3723,25609,38557,5646,2359,4913,6978,5208,34328,4396,8199,7733,626,6592,3932,8151,20781,8264,32187,4715,26379,1824,7442,7459,1295,4919,2982,32935,15149,1170,737,2514,6726,35797,29130,38577,35214,6815,8692,20486,1178,5509,4966,5464,2266,37060,4790,26173,7966,32720,4163,5351,14544,7363,2030,7728,2258,15328,19574,26472,13490,39095,25369,1319,34353,35448,27194,27001,20959,39683,1669,26983,10806,6619,17710,25561,26733,1482,7080,4257,6448,4647,13978,614,2501,40382,1194,5140,5657,537,32761,34321,6075,8035,33713,38604,14564,4651,25530,28001,25018,34200,1397,6849,6275,3311,1775,10716,20941,38496,5167,4025,2182,2041,4498,7678,40057,7386,7989,32460,8179,2394,19588,23492,19622,6645,5891,15153,4253,2877,6170,6033,5530,7606,12308,27364,15495,32465,19665,3152,32716,8691,6195,12458,5067,26841,6898,40446,32305,19316,21725,10513,2039,4597,2507,819,38726,17565,5393,8053,1171,8230,37216,37191,2630,765,3246,8028,39532,6451,26576,7232,2249,5010,32658,12209,28683,2785,6441,26745,24732,3002,34589,26678,3249,38563,32915,12498,26805,33084,21348,1598,24850,3355,32050,38591,17558,7385,40257,28054,1867,25411,32489,6591,6571,27014,1784,4816,8071,6326,7778,6106,7515,1143,3135,7019,3810,754,19647,6981,3306,17698,6973,7744,15317,26542,475,787,6099,6359,7265,7343,16313,1451,7645,2555,1317,30194,12940,28748,1361,6710,2319,4847,7358,12889,38646,4897,4373,4485,5329,2510,1481,1662,2660,44,6358,4987,10553,1455,1682,6604,38574,4943,4678,28663,7161,6141,35229,22441,7677,21264,35284,2268,3539,2945,476,613,5471,27039,1493,4862,968,32916,19424,15299,33080,7143,6044,1940,34407,6488,7175,6951,7508,32488,651,2837,38416,5822,5462,485,159,29260,4286,5671,7825,22920,19694,6794,6939,20974,2540,6077,34881,3550,6966,842,33755,37198,2959,8086,17624,7856,18423,6959,36598,20215,525,1494,6395,29645,23634,15240,4042,26261,31112,28241,6367,9783,23388,5449,12216,2382,11374,32164,3141,26761,1242,32379,20159,6270,8068,6944,4857,29525,34902,2699,32257,7162,32097,19690,1479,8215,7730,7400,34331,24279,40254,34484,16651,38535,20306,8069,39706,5495,3656,1028,18775,1472,29957,15484,3039,7810,2798,7043,12743,19642,1580,33677,2769,6823,1280,19663,6281,26764,200,1346,10498,28734,38462,15155,1576,6743,1085,35100,3401,347,5918,3606,25988,3259,24285,25781,7704,9763,593,1238,5651,32641,1718,2716,30281,32900,6173,16675,25522,2988,3623,6283,23837,3858,3638,6638,10590,4903,667,23625,37164,5978,26541,4863,38617,6290,29548,29980,37791,3463,2578,32996,12393,13143,749,17399,26537,13352,3455,277,3150,15411,1292,2375,6016,4439,12853,7773,7013,4322,17169,6577,684,4512,38515,26470,1128,26952,2646,588,24996,12753,26988,1799,22272,32674,7642,33462,20019,2170,25566,26645,30174,4638,26783,11755,4879,7315,1880,3533,6890,8250,2625,2853,3303,4956,33076,5461,6180,7855,7968,34890,6599,6053,1213,2689,3694,1835,2696,2886,7800,1681,491,26284,36369,5397,38456,5012,21648,26207,6356,4763,27862,1803,2413,8891,3641,32087,26602,32100,6995,37896,27453,5028,28723,6178,39659,16900,37373,21167,7090,19544,13620,2067,29010,37457,27174,13597,454,1543,36280,33334,28138,85,39699,24765,19638,31374,26653,5752,10872,27979,4392,27357,522,34551,5334,20209,2667,1225,12755,16228,2018,5647,38465,1678,32109,3208,10499,40335,37321,20709,26378,21011,40403,5823,6169,38625,4313,4125,3362,4682,24,2801,563,15487,13354,6019,27175,28999,30046,5582,1038,19497,6657,3667,27832,6431,4478,629,14281,21470,7814,15686,15125,2593,1964,12535,8180,25145,2476,2596,32588,26755,4935,19678,817,28205,4551,12547,3059,32405,28568,7669,27061,3233,3273,18937,35046,3464,24995,17155,10983,8276,4407,32464,4137,25064,908,33051,40088,18795,724,34473,27452,30483,30191,1328,19236,18803,40256,25460,4906,5890,17774,25641,38418,25459,39650,34815,32262,12539,6428,17261,7946,488,6761,170,2933,6062,36620,467,39995,26700,40318,33362,13579,26724,7173,5469,4181,37163,15298,1830,7868,5068,39663,4744,1434,28341,4328,37650,25818,28213,1316,11436,7405,25428,38502,26016,20369,6018,4552,2767,3797,37232,22271,3285,16042,7108,585,313,14505,1854,26466,26311,3305,414,27659,32069,12579,8613,48,31086,12000,13556,32331,3386,1700,2548,335,28451,32366,1962,4258,4893,4390,1097,28360,6036,4136,8358,35796,2529,17165,835,3680,3665,5242,28047,40468,24289,1324,27280,40454,16043,32290,25422,14056,34319,7009,7731,33576,22962,27000,19100,647,23628,1864,40472,8361,1569,3670,27109,15210,160,39968,17701,38667,3221,230,7342,6489,1901,1620,10223,26265,3364,22323,26363,11023,3318,6715,35051,37967,2880,6188,12745,3358,21214,5851,32662,4832,38868,8097,40439,2057,15500,7898,2235,4324,32822,11110,34708,19949,32260,296,3037,6562,29470,26550,6494,6444,9973,10205,26546,3331,5942,6968,2364,7619,12551,13663,33199,24254,461,1181,24874,6728,323,6299,38566,6467,11244,11438,2641,7693,690,15346,32192,8653,2138,1443,11036,5341,5403,16847,39110,6899,4365,25679,7051,18495,4642,6470,29682,38594,27990,2541,20577,7798,5800,1205,1572,26494,8062,32917,5463,27176,1422,3640,10201,36616,16792,2109,9607,33174,28355,6280,24269,32804,32943,17720,6273,2780,7565,8936,10221,745,2855,2952,26877,8177,28556,4060,10009,36494,28398,39103,1119,15264,2538,10837,38669,5610,5487,733,4986,27118,19387,5868,14890,27114,4348,29648,32615,9957,28376,8019,38848,32134,25563,20976,10767,32412,20061,25419,37212,28676,4044,12596,6727,1394,529,24293,5163,34405,15130,6838,27830,40514,5436,3513,520,531,11060,4023,37202,4300,8935,9971,40423,30196,38455,15493,21581,26707,5643,975,38839,584,2342,1440,40366,12509,32092,34273,12752,8569,7879,37125,15034,6390,3715,6050,14726,660,13002,36544,1918,16606,4965,22748,3315,10104,33403,22626,17676,33097,8650,4124,25299,361,7263,28083,1199,237,3402,26914,4497,30472,30690,24986,26348,910,16014,7387,7461,2207,3103,23438,32517,6851,13616,32568,35156,5118,610,26496,21778,9235,3366,38864,7030,1040,1892,23564,3292,3504,2502,11027,20440,32453,3861,5474,5263,27067,164,8231,9220,6566,1825,21357,6618,19874,40116,26735,32527,27123,32330,15557,7743,39260,38415,10812,1630,37209,8194,12,10870,9741,58,4908,33810,29011,3864,6338,25309,1347,5051,23191,39674,4958,19488,2412,4343,27545,8216,4126,7034,37285,26830,9768,36609,5869,4585,20901,19017,7369,19915,16371,26667,13598,26559,5494,6778,129,24089,27869,20441,6168,34420,422,39261,11944,5876,29850,5857,5544,3241,6612,38444,7447,15591,3719,40182,17658,34876,31552,4224,136,33682,7311,7288,26638,12480,2664,7742,6822,17690,27050,24887,26985,950,16346,4029,772,32747,666,3764,28029,37114,16909,256,35778,10192,1163,519,4945,19553,17880,6529,4589,6683,31850,13165,1833,9695,28391,35109,24005,3650,8182,14094,20809,2730,7803,32936,19199,17162,3356,27185,6391,1693,32120,16880,1913,31382,4563,14069,32731,161,18927,1315,27558,32225,38139,34419,26172,20953,20248,13531,6293,30170,1959,26958,40397,34476,8163,34912,29696,34323,7834,30189,345,301,39084,6241,15691,37330,32046,6052,2017,659,6767,26583,673,13708,384,27097,31434,16330,7584,320,32103,5606,33015,25458,6300,7941,26165,26971,6857,36737,29032,38638,123,26197,13985,27072,8162,32702,7388,2661,21761,8070,6100,1646,1846,3286,331,1134,23744,13802,36753,314,18792,37903,10717,10463,632,37797,750,25545,1432,7272,5281,28324,15217,17187,4293,34325,32297,26739,820,702,21822,1568,1800,6452,26527,13670,32494,36398,16164,26945,9798,27952,32923,29412,33783,12557,13668,4624,15028,4729,32388,15218,36562,17674,10552,17279,33879,2750,4165,2031,39311,1765,6616,2799,26202,13507,3675,2026,5617,38393,19268,40244,36541,30167,1297,26912,534,4197,7138,849,6816,8249,26838,36591,2871,5079,38608,22822,3346,7281,10974,1444,29856,38648,23662,7519,33157,23638,17452,26237,24833,2172,20031,12858,15241,4809,4825,6040,1753,2761,31233,18548,4918,17826,1096,5696,4723,18342,6043,40258,32190,16876,33120,12606,4176,27145,3449,424,6689,17185,5775,33855,22479,5183,6717,40243,7168,7793,4380,7816,17175,12728,2989,36656,28793,36836,29097,7112,10557,13700,32467,7741,1492,39361,289,4556,12521,32057,390,391,1279,22481,2409,25803,18263,24421,36047,40266,38628,8617,26924,27534,38572,8754,1000,11231,2146,26245,1993,15699,8226,23083,3520,11144,3500,24343,5006,5977,4096,20596,29959,4239,3345,32828,20738,3786,6634,24020,22369,28058,5858,30238,26168,32337,10391,3794,35326,1333,544,24150,38585,3308,40459,2592,9561,18754,1750,35814,7445,3817,32592,29399,32975,9149,5709,39828,28849,1999,17186,26597,18788,33166,12751,25438,26647,19692,33569,4611,38436,8817,6051,18419,8995,8014,9340,389,7046,1145,26326,10353,2969,5476,32258,38409,28558,26603,2065,28285,38642,37645,7632,3275,20902,24831,33647,20385,20018,7296,11543,3081,16589,30701,19494,7184,17460,31236,7397,37316,6396,23776,4628,26679,130,31162,15380,2232,38538,34581,2814,339,17144,32581,9756,15636,24977,32441,837,924,7005,34983,7411,14766,15729,2064,26205,12510,2001,793,13327,34192,38439,10560,6972,27906,17725,26715,29411,7817,9227,24167,10462,26572,29365,7783,26335,18412,32477,39368,28021,5687,1325,23668,21417,10133,2961,28678,24131,15485,15139,15488,40108,32705,6963,28383,27089,7513,26582,28582,33711,35039,29494,40132,4340,9767,27062,27010,40313,1041,14864,19393,26247,21304,26941,20975,10393,12205,1852,10234,5706,16510,37660,31154,40253,674,725,7018,13113,6758,10148,396,26994,19052,8188,28337,7188,33368,40040,1515,10810,13746,26736,2269,10668,26931,28367,7945,18612,27527,7286,682,35760,26553,38194,15642,411,3594,40133,36392,4730,11696,2191,4012,7421,27295,8184,16118,27020,36511,34349,12477,14288,9766,27130,33896,35049,35221,883,2212,7973,28333,4198,653,24722,26023,854,12707,27084,430,34344,14748,26199,2881,16025,8198,16899,2156,19853,5158,5485,16508,22961,27695,19626,1511,12758,10274,26514,39591,36406,38649,12121,3595,22247,29498,6873,2005,13954,28650,2915,5892,76,27148,2937,5798,29818,5704,2094,24040,30927,7264,1813,37211,40003,34994,37813,18000,21300,32783,34478,7832,33163,27650,2712,6780,25484,12513,898,4940,13631,28176,32338,25577,5419,3676,10236,7293,6942,38875,415,13639,34816,1524,30655,338,24039,6921,26798,37897,12456,7932,26210,22470,7318,33502,18496,14355]
    X_train_tfidf = X_train_tfidf.tocsr()[:,cols_to_use]
    X_dev_tfidf = X_dev_tfidf.tocsr()[:,cols_to_use]

    param_dict = {
              'colsample_bytree' : [0.6, 0.8],
              'learning_rate' : [0.01, 0.025, 0.5],
              'max_depth' : [3, 5],
              'min_child_weight' : [5],
              'n_estimators' : [10,30],
              'reg_alpha' : [0],
              'reg_lambda' : [10],
              'subsample' : [0.8, 1]
              }
    param_dict = {'max_depth' : [5],
                  'n_estimators' : [10],
                  'min_samples_split' : [10],
                  'min_samples_leaf' : [10]}
    param_dict = {
            'penalty' : ['l2'],
            'C' : [1.01, 10, 100, 1000]
    }
    param_space, param_to_int_dict = c_vars.get_param_space(param_dict)

    for param_list in param_space:
    # for i in [0.1]:
    # for i in [1]:
    # for i in [1.01]:
    # for i in np.logspace(-3, 0, num = 0 + 3 + 1):
    # for i in np.logspace(-1, 3, num = 1 + 3 + 1):
        print (param_list)
        # clf = RandomForestClassifier(max_depth=param_list[param_to_int_dict['max_depth']], 
                             # n_estimators=param_list[param_to_int_dict['n_estimators']],
                             # min_samples_split=param_list[param_to_int_dict['min_samples_split']],
                             # min_samples_leaf=param_list[param_to_int_dict['min_samples_leaf']],
                             # random_state = 42)
        # clf = XGBClassifier(colsample_bytree      = param_list[param_to_int_dict['colsample_bytree']],
                            # learning_rate         = param_list[param_to_int_dict['learning_rate']],
                            # max_depth             = param_list[param_to_int_dict['max_depth']],
                            # min_child_weight      = param_list[param_to_int_dict['min_child_weight']],
                            # n_estimators          = param_list[param_to_int_dict['n_estimators']],
                            # reg_alpha             = param_list[param_to_int_dict['reg_alpha']],
                            # reg_lambda            = param_list[param_to_int_dict['reg_lambda']],
                            # subsample             = param_list[param_to_int_dict['subsample']])
        # clf = GradientBoostingClassifier(
                # max_depth        = 7,
                # n_estimators     = 40,
                # learning_rate    = 1,
                # random_state = 42)
        # clf = MLPClassifier(activation         = 'logistic',
                            # hidden_layer_sizes = (200, 50, 10),
                            # learning_rate      = 'invscaling',
                            # max_iter           = 200,
                            # solver             = 'adam',
                            # random_state = 42)
        # clf = MultinomialNB(alpha = i)
        # clf = GaussianNB()
        # clf = SVC(C = i)
        clf = LogisticRegression(penalty = param_list[param_to_int_dict['penalty']],
                                 C = param_list[param_to_int_dict['C']])
                                 # class_weight = 'balanced')
        # clf = RFE(clf, n_features_to_select = 300, step = 100, verbose = 1)
        # if type(X_train_tfidf) is not np.ndarray:
            # X_train_tfidf = X_train_tfidf.toarray()
            # X_dev_tfidf = X_dev_tfidf.toarray()

        clf.fit(X_train_tfidf, y_train)
        # print (clf.coef_)
        # print (clf.intercept_)
        # print (clf.feature_importances_)

        y_pred = clf.predict(X_dev_tfidf)
        y_pred_proba = clf.predict_proba(X_dev_tfidf)[:,1]
        y_pred_train = clf.predict(X_train_tfidf)
        y_pred_proba_train = clf.predict_proba(X_train_tfidf)[:,1]

        print ('Train, ' + str(accuracy_score(y_train, y_pred_train)) + ',' + str(roc_auc_score(y_train, y_pred_train)))
        print ('Test, ' + str(accuracy_score(y_dev, y_pred)) + ',' + str(roc_auc_score(y_dev, y_pred)))
        # print ('Train, ' + str(accuracy_score(y_train, y_pred_train)) + ',')
        # print ('Test, ' + str(accuracy_score(y_dev, y_pred)) + ',')
        '''
        with open ('../analysis/ranking_', 'wb') as f:
            pickle.dump(clf.ranking_, f)
        np.savetxt('../analysis/test.out', clf.ranking_, delimiter=',')
        print (clf.ranking_)
        sys.exit()
        '''
        df_dev['y_dev'] = y_dev
        df_dev['y_pred'] = y_pred
        df_dev['y_pred_proba'] = y_pred_proba
        # df_dev['conf'] = clf.decision_function(X_dev_tfidf)
        df_dev.to_csv('../analysis/dev_analysis.csv', index = False)
        '''
        values = X_train_tfidf.max(0).toarray()[0]
        feature_names = np.hstack((np.array(tfVect1.get_feature_names()), np.array(tfVect2.get_feature_names()), np.array(tfVect3.get_feature_names())))
        print (feature_names.shape)
        features_series = pd.Series(values, index = feature_names)

        f = open('../analysis/corr.csv', 'w')
        for i in range(X_train_tfidf.shape[1]):
            f.write (str(features_series.index[i]) + ',' + str(features_series[i]) + ',' + str(i) + ',' + str(pearsonr(X_train_tfidf.toarray()[:,i], y_train)[0]) + '\n')
        f.close()
        '''
        # top_20 = features_series.nlargest(20)
        # bot_20 = features_series.nsmallest(20)
        # print ('bot_20')
        # print (bot_20)
        # print ('top_20')
        # print (top_20)

    
    # delete training data to clean up memory
    del X_train
    del X_train_tfidf
    del df_train
    del X_dev
    del X_dev_tfidf
    del df_dev
    del y_train
    del y_dev
    del y_pred
    del y_pred_proba
    del y_pred_train
    del y_pred_proba_train
    gc.collect()
    sys.exit()

    
    # predict on the submit set
    df_submit = pd.read_csv(c_vars.test_file_processed, encoding = "ISO-8859-1")
    df_submit['Description_Clean'].fillna('', inplace = True, axis = 0)
    df_submit['Description_Clean_Adj'].fillna('', inplace = True, axis = 0)

    df_submit['text_length'] = df_submit['Description_Clean'].apply(lambda x: len(x))
    df_submit['word_count'] = df_submit['Description_Clean'].apply(lambda x: len(x.split(' ')))

    df_submit = pd.merge(df_submit, df_device, how = 'left', on = 'Device_Used', suffixes = ('', ''))
    X_submit = df_submit[['Description_Clean', 'Description_Clean_Adj', 'text_length', 'word_count', 'target_rate']].as_matrix()
    # X_submit_tfidf = hstack((tfVect1.transform(X_submit[:, 0]), tfVect2.transform(X_submit[:, 0]), tfVect3.transform(X_submit[:, 1])))
    X_submit_tfidf = hstack((tfVect1.transform(X_submit[:, 0]), tfVect2.transform(X_submit[:, 0]), tfVect3.transform(X_submit[:, 1])))
    # X_submit_tfidf = X_submit_tfidf.toarray()
    # X_submit_tfidf = truncatedsvd.transform(X_submit_tfidf)
    # X_submit_tfidf = tfVect.transform(X_submit[:, 0])
    # X_submit_tfidf = c_vars.add_feature(X_submit_tfidf, X_submit[:, 2].astype(np.float64))
    # X_submit_tfidf = c_vars.add_feature(X_submit_tfidf, X_submit[:, 3].astype(np.int64))
    # X_submit_tfidf = c_vars.add_feature(X_submit_tfidf, X_submit[:, 4].astype(np.int64))

    y_pred_submit = clf.predict(X_submit_tfidf)
    df_submit['Is_Response'] = y_pred_submit
    df_submit['Is_Response'] = df_submit['Is_Response'].apply(lambda x: 'happy' if x == 1 else 'not_happy')
    df_submit[['User_ID', 'Is_Response']].to_csv('../output/submit_20171118_0938_1_lr.csv', index = False)
    

if __name__ == '__main__':
    main()